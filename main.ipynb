{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation and analysis\n",
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Import libraries for machine learning and deep learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # For splitting data into train and test sets\n",
    "from sklearn.ensemble import RandomForestClassifier  # For Random Forest model\n",
    "import tensorflow as tf  # For building deep learning models\n",
    "from tensorflow.keras.models import Sequential  # For building sequential models\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    ")  # For defining layers in neural networks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint  # Callbacks for training\n",
    "from tensorflow.keras.optimizers import Adam  # Optimizer for neural networks\n",
    "\n",
    "# Import libraries for evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")  # For model evaluation\n",
    "\n",
    "# Import custom utility functions from a script\n",
    "from scripts.utils import create_sequences, preprocess, perform_anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets:\n",
    "\n",
    "Botnet Class Labels\n",
    "- **clear**\n",
    "- **neris**\n",
    "- **rbot**\n",
    "- **fast flux**\n",
    "- **donbot**\n",
    "- **qvod**\n",
    "\n",
    "ToN IoT Class Labels\n",
    "- **clear**\n",
    "- **backdoor**\n",
    "- **dos**\n",
    "- **injection**\n",
    "- **mitm**\n",
    "- **password**\n",
    "- **ransomware**\n",
    "- **scanning**\n",
    "- **xss**\n",
    "\n",
    "### Download Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'botnet_multiclass': 'https://zenodo.org/records/8035724/files/botnet_multiclass.csv?download=1',\n",
    "    'ton_iot_multiclass': 'https://zenodo.org/records/8035724/files/ton_iot_multiclass.csv?download=1',\n",
    "}\n",
    "\n",
    "save_dir = 'datasets/'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    for filename, url in urls.items():\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(save_dir, filename + '.csv'), 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}\")\n",
    "\n",
    "df_primary = pd.read_csv('datasets/botnet_multiclass.csv')\n",
    "# df_secondary = pd.read_csv('datasets/ton_iot_multiclass.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA\n",
    "- Low P Value: statistical significance\n",
    "- High V Value: seperation from group mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>PEARSON_SK1_SKEWNESS</th>\n",
       "      <th>PEARSON_SK2_SKEWNESS</th>\n",
       "      <th>FISHER_MI_3_SKEWNESS</th>\n",
       "      <th>ENTROPY</th>\n",
       "      <th>SCALED_ENTROPY</th>\n",
       "      <th>HURST_EXPONENT</th>\n",
       "      <th>P_BENFORD</th>\n",
       "      <th>TIME_DISTRIBUTION</th>\n",
       "      <th>AREA_VALUES_DISTRIBUTION</th>\n",
       "      <th>...</th>\n",
       "      <th>SPECTRAL_ENERGY</th>\n",
       "      <th>POWER_MEAN</th>\n",
       "      <th>SPECTRAL_FLUX</th>\n",
       "      <th>POWER_STD</th>\n",
       "      <th>MAX_POWER</th>\n",
       "      <th>CNT_ZEROS</th>\n",
       "      <th>SPECTRAL_CREST</th>\n",
       "      <th>SPECTRAL_ENTROPY</th>\n",
       "      <th>SPECTRAL_CENTROID</th>\n",
       "      <th>GALTON_SKEWNESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F-Value</th>\n",
       "      <td>1502.211269</td>\n",
       "      <td>8743.866331</td>\n",
       "      <td>2608.909682</td>\n",
       "      <td>2664.922306</td>\n",
       "      <td>4036.56304</td>\n",
       "      <td>12086.719545</td>\n",
       "      <td>6088.641145</td>\n",
       "      <td>4249.979261</td>\n",
       "      <td>2151.918931</td>\n",
       "      <td>9860.27935</td>\n",
       "      <td>...</td>\n",
       "      <td>1.644773</td>\n",
       "      <td>1.644709</td>\n",
       "      <td>1.339223</td>\n",
       "      <td>1.226269</td>\n",
       "      <td>1.207071</td>\n",
       "      <td>1.049670</td>\n",
       "      <td>0.797632</td>\n",
       "      <td>0.777401</td>\n",
       "      <td>0.194402</td>\n",
       "      <td>0.175223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P-Value</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144348</td>\n",
       "      <td>0.144364</td>\n",
       "      <td>0.244262</td>\n",
       "      <td>0.293668</td>\n",
       "      <td>0.302814</td>\n",
       "      <td>0.386333</td>\n",
       "      <td>0.551129</td>\n",
       "      <td>0.565806</td>\n",
       "      <td>0.964790</td>\n",
       "      <td>0.971926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1  PEARSON_SK1_SKEWNESS  PEARSON_SK2_SKEWNESS  \\\n",
       "F-Value   1502.211269           8743.866331           2608.909682   \n",
       "P-Value      0.000000              0.000000              0.000000   \n",
       "\n",
       "         FISHER_MI_3_SKEWNESS     ENTROPY  SCALED_ENTROPY  HURST_EXPONENT  \\\n",
       "F-Value           2664.922306  4036.56304    12086.719545     6088.641145   \n",
       "P-Value              0.000000     0.00000        0.000000        0.000000   \n",
       "\n",
       "           P_BENFORD  TIME_DISTRIBUTION  AREA_VALUES_DISTRIBUTION  ...  \\\n",
       "F-Value  4249.979261        2151.918931                9860.27935  ...   \n",
       "P-Value     0.000000           0.000000                   0.00000  ...   \n",
       "\n",
       "         SPECTRAL_ENERGY  POWER_MEAN  SPECTRAL_FLUX  POWER_STD  MAX_POWER  \\\n",
       "F-Value         1.644773    1.644709       1.339223   1.226269   1.207071   \n",
       "P-Value         0.144348    0.144364       0.244262   0.293668   0.302814   \n",
       "\n",
       "         CNT_ZEROS  SPECTRAL_CREST  SPECTRAL_ENTROPY  SPECTRAL_CENTROID  \\\n",
       "F-Value   1.049670        0.797632          0.777401           0.194402   \n",
       "P-Value   0.386333        0.551129          0.565806           0.964790   \n",
       "\n",
       "         GALTON_SKEWNESS  \n",
       "F-Value         0.175223  \n",
       "P-Value         0.971926  \n",
       "\n",
       "[2 rows x 78 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anova_df = perform_anova(df = df_primary, target_column = 'LABEL')\n",
    "anova_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Values for Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(clear        37983\n",
       " qvod           277\n",
       " fast_flux      166\n",
       " donbot          27\n",
       " Name: LABEL, dtype: int64,\n",
       " clear    15354\n",
       " neris     6176\n",
       " Name: LABEL, dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_first = df_primary.copy()\n",
    "primary_first['TIME_FIRST'] = pd.to_datetime(df_primary['TIME_FIRST'], unit='s')\n",
    "\n",
    "primary_second = primary_first.copy()\n",
    "\n",
    "date_one = pd.to_datetime('2011-08-16').date()\n",
    "date_two = pd.to_datetime('2011-08-10').date()\n",
    "\n",
    "primary_first = primary_first[primary_first['TIME_FIRST'].dt.date == date_one]\n",
    "primary_second = primary_second[primary_second['TIME_FIRST'].dt.date == date_two]\n",
    "\n",
    "primary_first['LABEL'].value_counts(), primary_second['LABEL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'PACKETS',\n",
    "    'PACKETS_REV',\n",
    "    'BYTES',\n",
    "    'BYTES_REV',\n",
    "    'DURATION',\n",
    "    'BURSTINESS',\n",
    "    'ENTROPY',\n",
    "    'SCALED_ENTROPY',\n",
    "    'HURST_EXPONENT',\n",
    "    'CNT_ZEROS',\n",
    "    'CNT_NZ_DISTRIBUTION',\n",
    "    'TIME_DISTRIBUTION',\n",
    "    'PEARSON_SK1_SKEWNESS',\n",
    "    'LABEL'\n",
    "]\n",
    "\n",
    "df_primary_processed = preprocess(df = df_primary, columns_to_keep = columns_to_keep, IP = '147.32.84.165', days = ['2011-08-16'])\n",
    "df_secondary_processed = preprocess(df = df_primary, columns_to_keep = columns_to_keep, IP = '147.32.84.165', days = ['2011-08-10'])\n",
    "# df_secondary_processed = preprocess(df = df_secondary, columns_to_keep = columns_to_keep, IP = '192.168.1.195', days = ['2019-04-03', '2019-04-26'], sample_size = df_primary_processed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vizualization\n",
    "- **Boxen**\n",
    "- **KDE**\n",
    "- **Histogram**\n",
    "- **Pairplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_features = df_primary_processed.drop('LABEL', axis=1)\n",
    "\n",
    "# Iterating through each column to create Boxen, KDE, and Histogram plots and save them\n",
    "for col in df_features.columns:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Boxen plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.boxenplot(data=df_features, x=col)\n",
    "    plt.title(f'Boxenplot of {col}')\n",
    "\n",
    "    # KDE plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.kdeplot(data=df_features, x=col, fill=True)\n",
    "    plt.title(f'KDE Plot of {col}')\n",
    "\n",
    "    # Histogram of 'LABEL'\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.histplot(data=df_primary_processed, x='LABEL', bins=10, kde=False)\n",
    "    plt.title('Histogram of LABEL')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'viz/{col}.png')\n",
    "    plt.close()\n",
    "\n",
    "plt.figure()\n",
    "sns.pairplot(df_features)\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz/pairplot.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model\n",
    "\n",
    "### Data Preparation\n",
    "- Data is prepared for the Random Forest model by separating the target variable 'LABEL' from the features.\n",
    "- The dataset is split into training and testing sets using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for Random Forest\n",
    "X_rf = df_primary_processed.drop('LABEL', axis=1)\n",
    "y_rf = df_primary_processed['LABEL']\n",
    "\n",
    "# Splitting the dataset into training and testing sets for Random Forest\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "- A Random Forest classifier with 100 decision trees is created using the `RandomForestClassifier` from scikit-learn.\n",
    "- The classifier is trained on the training data using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters found by GridSearchCV: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200],  # Different number of trees in the forest\n",
    "    'max_depth': [5, 10, None],       # Maximum number of levels in each decision tree\n",
    "    'min_samples_split': [2, 4, 6],   # Minimum number of data points placed in a node before the node is split\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum number of data points allowed in a leaf node\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Best parameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found by GridSearchCV:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_split=6, random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    random_state=42  \n",
    ")\n",
    "\n",
    "rf_classifier.fit(X_train_rf, y_train_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "- Predictions are made on the test data using the trained Random Forest model. These predictions are stored in the variable `y_pred_rf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with Random Forest\n",
    "y_pred_rf = rf_classifier.predict(X_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "- To assess the model's performance, several evaluation metrics are calculated:\n",
    "  - **Accuracy**: Measures the proportion of correctly classified instances.\n",
    "  - **Precision**: Measures the ability to correctly identify positive cases.\n",
    "  - **Recall**: Measures the ability to find all positive cases.\n",
    "  - **F1 Score**: Combines precision and recall into a single metric.\n",
    "  - **ROC-AUC**: Measures the area under the Receiver Operating Characteristic curve, indicating the model's ability to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model:\n",
      "Accuracy: 0.9995787700084247\n",
      "Precision: 0.9896907216494846\n",
      "Recall: 0.9896907216494846\n",
      "F1 Score: 0.9896907216494846\n",
      "ROC-AUC: 0.9947378570621106\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\n",
    "precision_rf = precision_score(y_test_rf, y_pred_rf)\n",
    "recall_rf = recall_score(y_test_rf, y_pred_rf)\n",
    "f1_rf = f1_score(y_test_rf, y_pred_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test_rf, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Model:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(f\"Precision: {precision_rf}\")\n",
    "print(f\"Recall: {recall_rf}\")\n",
    "print(f\"F1 Score: {f1_rf}\")\n",
    "print(f\"ROC-AUC: {roc_auc_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM Model\n",
    "\n",
    "### Data Preparation\n",
    "- Sequences are created from the primary dataset with a time step of 1.\n",
    "- The dataset is then split into training and testing sets using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sequences for Basic LSTM\n",
    "n_steps_basic = 1\n",
    "X_lstm_basic, y_lstm_basic = create_sequences(df_primary_processed, n_steps_basic)\n",
    "\n",
    "# Splitting the dataset into training and testing sets for Basic LSTM\n",
    "X_train_basic, X_test_basic, y_train_basic, y_test_basic = train_test_split(X_lstm_basic, y_lstm_basic, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "- A Basic LSTM model is constructed using Keras with the following architecture:\n",
    "  - A single LSTM layer with 10 units and a linear activation function.\n",
    "  - A Dense layer with a sigmoid activation function for binary classification.\n",
    "- The Adam optimizer with a learning rate of 0.1 is used.\n",
    "\n",
    "### Model Training\n",
    "- The Basic LSTM model is trained on the training data using the `fit` method with specified epochs and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 2s 1ms/step - loss: 0.0190 - accuracy: 0.9811 - val_loss: 0.0236 - val_accuracy: 0.9762\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 1s 867us/step - loss: 0.0185 - accuracy: 0.9815 - val_loss: 0.0175 - val_accuracy: 0.9825\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 1s 876us/step - loss: 0.0182 - accuracy: 0.9818 - val_loss: 0.0183 - val_accuracy: 0.9817\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 1s 870us/step - loss: 0.0174 - accuracy: 0.9826 - val_loss: 0.0179 - val_accuracy: 0.9821\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 1s 903us/step - loss: 0.0173 - accuracy: 0.9827 - val_loss: 0.0179 - val_accuracy: 0.9821\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 1s 879us/step - loss: 0.0172 - accuracy: 0.9828 - val_loss: 0.0179 - val_accuracy: 0.9821\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 1s 870us/step - loss: 0.0172 - accuracy: 0.9828 - val_loss: 0.0179 - val_accuracy: 0.9821\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 1s 876us/step - loss: 0.0185 - accuracy: 0.9814 - val_loss: 0.0179 - val_accuracy: 0.9821\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 1s 911us/step - loss: 0.0202 - accuracy: 0.9798 - val_loss: 0.0200 - val_accuracy: 0.9800\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 1s 865us/step - loss: 0.0200 - accuracy: 0.9800 - val_loss: 0.0200 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb025b0f40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the Basic LSTM Model\n",
    "model_basic = Sequential()\n",
    "model_basic.add(LSTM(10, activation='linear', input_shape=(n_steps_basic, X_train_basic.shape[2])))\n",
    "model_basic.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Large learning rate for Basic LSTM\n",
    "adam_basic = Adam(learning_rate=0.1)\n",
    "\n",
    "model_basic.compile(optimizer=adam_basic, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Fit Basic LSTM model\n",
    "model_basic.fit(X_train_basic, y_train_basic, epochs=10, batch_size=32, validation_data=(X_test_basic, y_test_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "- After training, predictions are made on the test data using the trained Basic LSTM model. Predictions are rounded to obtain binary classification results, which are stored in `y_pred_basic`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 0s 555us/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions with Basic LSTM\n",
    "y_pred_basic = np.round(model_basic.predict(X_test_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "- To assess the performance of the model, several evaluation metrics are calculated:\n",
    "  - **Accuracy**: Measures the proportion of correctly classified instances.\n",
    "  - **Precision**: Measures the ability to correctly identify positive cases.\n",
    "  - **Recall**: Measures the ability to find all positive cases.\n",
    "  - **F1 Score**: Combines precision and recall into a single metric.\n",
    "  - **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic LSTM Model:\n",
      "Accuracy: 0.9799915754001685\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "ROC-AUC: 0.49978517722878624\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Basic LSTM model\n",
    "accuracy_basic = accuracy_score(y_test_basic, y_pred_basic)\n",
    "precision_basic = precision_score(y_test_basic, y_pred_basic,  zero_division=0)\n",
    "recall_basic = recall_score(y_test_basic, y_pred_basic,  zero_division=0)\n",
    "f1_basic = f1_score(y_test_basic, y_pred_basic)\n",
    "roc_auc_basic = roc_auc_score(y_test_basic, y_pred_basic)\n",
    "\n",
    "print(\"Basic LSTM Model:\")\n",
    "print(f\"Accuracy: {accuracy_basic}\")\n",
    "print(f\"Precision: {precision_basic}\")\n",
    "print(f\"Recall: {recall_basic}\")\n",
    "print(f\"F1 Score: {f1_basic}\")\n",
    "print(f\"ROC-AUC: {roc_auc_basic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Between Basic and Enhanced LSTM Models\n",
    "\n",
    "- **Time Steps**:\n",
    "  - In the basic model, the time step was set to 1.\n",
    "  - In the enhanced model, the time step was increased to 5 for more complex sequence learning.\n",
    "\n",
    "- **LSTM Units**: \n",
    "  - Basic model uses only 10 units with linear activation.\n",
    "  - Enhanced model uses 100 units in a Bidirectional LSTM with `tanh` activation, followed by another LSTM layer with 50 units and `relu` activation.\n",
    "\n",
    "- **Learning Rate**: \n",
    "  - Basic model uses a high learning rate (0.1).\n",
    "  - Enhanced model uses the default learning rate of the Adam optimizer.\n",
    "\n",
    "- **Loss Function**: \n",
    "  - Basic model uses mean squared error (MSE), not typical for binary classification.\n",
    "  - Enhanced model uses binary crossentropy.\n",
    "\n",
    "- **Regularization and Normalization**: \n",
    "  - Basic model lacks additional layers for regularization.\n",
    "  - Enhanced model includes Dropout and Batch Normalization layers.\n",
    "\n",
    "- **Model Architecture**: \n",
    "  - Basic model is simpler with fewer layers.\n",
    "  - Enhanced model is more complex with Bidirectional and stacked LSTM layers.\n",
    "\n",
    "- **Callbacks**: \n",
    "  - Basic model does not use any callbacks.\n",
    "  - Enhanced model employs Early Stopping and Model Checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sequences for Enhanced LSTM\n",
    "n_steps_enhanced = 5\n",
    "X_lstm_enhanced, y_lstm_enhanced = create_sequences(df_primary_processed, n_steps_enhanced)\n",
    "\n",
    "# Splitting the dataset into training and testing sets for Enhanced LSTM\n",
    "X_train_enhanced, X_test_enhanced, y_train_enhanced, y_test_enhanced = train_test_split(X_lstm_enhanced, y_lstm_enhanced, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "594/594 [==============================] - 9s 8ms/step - loss: 0.0488 - accuracy: 0.9854 - val_loss: 0.0318 - val_accuracy: 0.9905\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0323 - accuracy: 0.9891 - val_loss: 0.0327 - val_accuracy: 0.9878\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0314 - accuracy: 0.9890 - val_loss: 0.0324 - val_accuracy: 0.9909\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 0.0359 - val_accuracy: 0.9888\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0299 - accuracy: 0.9896 - val_loss: 0.0312 - val_accuracy: 0.9903\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0287 - accuracy: 0.9893 - val_loss: 0.0282 - val_accuracy: 0.9901\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0283 - accuracy: 0.9897 - val_loss: 0.0315 - val_accuracy: 0.9905\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0272 - accuracy: 0.9898 - val_loss: 0.0281 - val_accuracy: 0.9909\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0288 - accuracy: 0.9900 - val_loss: 0.0325 - val_accuracy: 0.9901\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 3s 5ms/step - loss: 0.0265 - accuracy: 0.9902 - val_loss: 0.0325 - val_accuracy: 0.9899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb064ec2e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the Enhanced LSTM Model\n",
    "model_enhanced = Sequential()\n",
    "model_enhanced.add(Bidirectional(LSTM(100, return_sequences=True, activation='tanh'), input_shape=(n_steps_enhanced, X_train_enhanced.shape[2])))\n",
    "model_enhanced.add(Dropout(0.2))\n",
    "model_enhanced.add(BatchNormalization())\n",
    "model_enhanced.add(LSTM(50, activation='relu'))\n",
    "model_enhanced.add(Dense(1, activation='sigmoid'))\n",
    "model_enhanced.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for Enhanced LSTM\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# Fit Enhanced LSTM model\n",
    "model_enhanced.fit(X_train_enhanced, y_train_enhanced, epochs=10, batch_size=32, validation_data=(X_test_enhanced, y_test_enhanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions with Enhanced LSTM\n",
    "y_pred_enhanced = np.round(model_enhanced.predict(X_test_enhanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced LSTM Model:\n",
      "Accuracy: 0.9898883505371814\n",
      "Precision: 0.7901234567901234\n",
      "Recall: 0.6736842105263158\n",
      "F1 Score: 0.7272727272727273\n",
      "ROC-AUC: 0.8350149341539574\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Enhanced LSTM model\n",
    "accuracy_enhanced = accuracy_score(y_test_enhanced, y_pred_enhanced)\n",
    "precision_enhanced = precision_score(y_test_enhanced, y_pred_enhanced)\n",
    "recall_enhanced = recall_score(y_test_enhanced, y_pred_enhanced)\n",
    "f1_enhanced = f1_score(y_test_enhanced, y_pred_enhanced)\n",
    "roc_auc_enhanced = roc_auc_score(y_test_enhanced, y_pred_enhanced)\n",
    "\n",
    "print(\"Enhanced LSTM Model:\")\n",
    "print(f\"Accuracy: {accuracy_enhanced}\")\n",
    "print(f\"Precision: {precision_enhanced}\")\n",
    "print(f\"Recall: {recall_enhanced}\")\n",
    "print(f\"F1 Score: {f1_enhanced}\")\n",
    "print(f\"ROC-AUC: {roc_auc_enhanced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Dataset Enhanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 0s 2ms/step\n",
      "Enhanced Secondary LSTM Model Evaluation:\n",
      "Accuracy: 0.7310946589106293\n",
      "Precision: 0.8865546218487395\n",
      "Recall: 0.17568692756036636\n",
      "F1 Score: 0.29325920778318276\n",
      "ROC-AUC: 0.5826129329781684\n"
     ]
    }
   ],
   "source": [
    "# Creating sequences for Enhanced Secondary LSTM\n",
    "n_steps_enhanced_secondary = 5\n",
    "X_lstm_enhanced_secondary, y_lstm_enhanced_secondary = create_sequences(df_secondary_processed, n_steps_enhanced_secondary)\n",
    "\n",
    "# Splitting the dataset into training and testing sets for Enhanced Secondary LSTM\n",
    "X_train_enhanced_secondary, X_test_enhanced_secondary, y_train_enhanced_secondary, y_test_enhanced_secondary = train_test_split(X_lstm_enhanced_secondary, y_lstm_enhanced_secondary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Predictions using the Enhanced Secondary LSTM model\n",
    "y_pred_enhanced_secondary = np.round(model_enhanced.predict(X_test_enhanced_secondary))\n",
    "\n",
    "# Evaluate the Enhanced Secondary LSTM model\n",
    "accuracy_enhanced_secondary = accuracy_score(y_test_enhanced_secondary, y_pred_enhanced_secondary)\n",
    "precision_enhanced_secondary = precision_score(y_test_enhanced_secondary, y_pred_enhanced_secondary, zero_division=0)\n",
    "recall_enhanced_secondary = recall_score(y_test_enhanced_secondary, y_pred_enhanced_secondary, zero_division=0)\n",
    "f1_enhanced_secondary = f1_score(y_test_enhanced_secondary, y_pred_enhanced_secondary)\n",
    "roc_auc_enhanced_secondary = roc_auc_score(y_test_enhanced_secondary, y_pred_enhanced_secondary)\n",
    "\n",
    "print(\"Enhanced Secondary LSTM Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_enhanced_secondary}\")\n",
    "print(f\"Precision: {precision_enhanced_secondary}\")\n",
    "print(f\"Recall: {recall_enhanced_secondary}\")\n",
    "print(f\"F1 Score: {f1_enhanced_secondary}\")\n",
    "print(f\"ROC-AUC: {roc_auc_enhanced_secondary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Dataset Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Evaluation for the Secondary Dataset:\n",
      "Accuracy: 0.8567274649748876\n",
      "Precision: 1.0\n",
      "Recall: 0.5564648117839607\n",
      "F1 Score: 0.7150368033648791\n",
      "ROC-AUC: 0.7782324058919803\n"
     ]
    }
   ],
   "source": [
    "# Preparing data for Random Forest\n",
    "X_rf_secondary = df_secondary_processed.drop('LABEL', axis=1)\n",
    "y_rf_secondary = df_secondary_processed['LABEL']\n",
    "\n",
    "# Splitting the dataset into training and testing sets for Random Forest\n",
    "X_train_rf_secondary , X_test_rf_secondary , y_train_rf_secondary , y_test_rf_secondary = train_test_split(X_rf_secondary, y_rf_secondary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Predictions using the Random Forest model for the secondary dataset\n",
    "y_pred_rf_secondary = rf_classifier.predict(X_test_rf_secondary)\n",
    "\n",
    "# Evaluate the Random Forest model for the secondary dataset\n",
    "accuracy_rf_secondary = accuracy_score(y_test_rf_secondary, y_pred_rf_secondary)\n",
    "precision_rf_secondary = precision_score(y_test_rf_secondary, y_pred_rf_secondary, zero_division=0)\n",
    "recall_rf_secondary = recall_score(y_test_rf_secondary, y_pred_rf_secondary, zero_division=0)\n",
    "f1_rf_secondary = f1_score(y_test_rf_secondary, y_pred_rf_secondary)\n",
    "roc_auc_rf_secondary = roc_auc_score(y_test_rf_secondary, y_pred_rf_secondary)\n",
    "\n",
    "print(\"Random Forest Model Evaluation for the Secondary Dataset:\")\n",
    "print(f\"Accuracy: {accuracy_rf_secondary}\")\n",
    "print(f\"Precision: {precision_rf_secondary}\")\n",
    "print(f\"Recall: {recall_rf_secondary}\")\n",
    "print(f\"F1 Score: {f1_rf_secondary}\")\n",
    "print(f\"ROC-AUC: {roc_auc_rf_secondary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
